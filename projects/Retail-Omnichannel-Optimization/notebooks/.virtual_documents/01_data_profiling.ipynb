"""
01_data_profiling.ipynb
======================
Pierwsza analiza danych dla Retail Omnichannel Optimization
Autor: Pawe≈Ç ≈ªo≈ÇƒÖdkiewicz
Data: 12 sierpnia 2025

Cel: Poznanie datasetu, identyfikacja problem√≥w jako≈õciowych,
     przygotowanie do budowy modelu gwiazdy.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("üöÄ Rozpoczynamy analizƒô Online Retail II dataset")
print("=" * 50)
print(f"Python version: {pd.__version__}")
print(f"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M')}")



# =======================================
# SPRAWDZENIE ZAWARTO≈öCI PLIKU EXCEL
# =======================================

import pandas as pd

print("ANALIZA PLIKU EXCEL")
print("=" * 35)

file_path = '../data/raw/online_retail_II.xlsx'

# Sprawd≈∫ jakie zak≈Çadki sƒÖ w pliku
xl_file = pd.ExcelFile(file_path)
sheet_names = xl_file.sheet_names

print(f"Znalezione zak≈Çadki: {sheet_names}")
print(f"Liczba zak≈Çadek: {len(sheet_names)}")

# Sprawd≈∫ rozmiar ka≈ºdej zak≈Çadki
for sheet in sheet_names:
    temp_df = pd.read_excel(file_path, sheet_name=sheet)
    print(f"* {sheet}: {len(temp_df):,} rekord√≥w")
    print(f"* Zakres dat: {temp_df['InvoiceDate'].min()} -> {temp_df['InvoiceDate'].max()}")



# ===========================================================
# WCZYTANIE DATASETU ONLINE RETAIL II - Z ANALIZƒÑ DUPLIKAT√ìW  
# ===========================================================

print("Wczytywanie Online Retail II dataset...")

# ≈öcie≈ºka wzglƒôdna z notebooks folder
file_path = '../data/raw/online_retail_II.xlsx'

try:
    # Sprawd≈∫ dostƒôpne arkusze
    xl_file = pd.ExcelFile(file_path)
    sheet_names = xl_file.sheet_names
    print(f"Dostƒôpne arkusze: {sheet_names}")

    # Wczytaj oba arkusze
    sheets_data = {}
    for sheet in sheet_names:
        sheets_data[sheet] = pd.read_excel(file_path, sheet_name=sheet)
        print(f"\nArkusz '{sheet}':")
        print(f"   - Rekord√≥w: {len(sheets_data[sheet]):,}")
        print(f"   - Kolumny: {list(sheets_data[sheet].columns)}")
        print(f"   - Zakres dat: {sheets_data[sheet]['InvoiceDate'].min()} ‚Üí {sheets_data[sheet]['InvoiceDate'].max()}")

    # Analiza nak≈ÇadajƒÖcych siƒô danych (duplikaty)
    print(f"\n" + "=" * 50)
    print("ANALIZA NAK≈ÅADAJƒÑCYCH SIƒò DANYCH (DUPLIKATY)")
    print("=" * 50)

    if len(sheet_names) == 2:
        sheet1_name, sheet2_name = sheet_names[0], sheet_names[1]
        df1, df2 = sheets_data[sheet1_name], sheets_data[sheet2_name]

        # Klucz do weryfikacji pod kƒÖtem duplikat√≥w
        def create_transaction_key(df):
            return (df['Invoice'].astype(str) + "_" + 
                    df['StockCode'].astype(str) + "_" + 
                    df['Quantity'].astype(str) + "_" +
                    pd.to_datetime(df['InvoiceDate']).dt.strftime('%Y%m%d_%H%M') + "_" +
                    df['Price'].astype(str))

        df1['full_transaction_key'] = create_transaction_key(df1)
        df2['full_transaction_key'] = create_transaction_key(df2)

        # Sprtawd≈∫ duplikaty WEWNƒÑTRZ ka≈ºdego arkusza

        df1_internal_dupes = df1[df1.duplicated(subset=['full_transaction_key'], keep=False)]
        df2_internal_dupes = df2[df2.duplicated(subset=['full_transaction_key'], keep=False)]

        print(f"DUPLIKATY WEWNƒòTRZNE:")
        print(f"Arkusz 1 wewnƒôtrzne duplikaty: {len(df1_internal_dupes):,}")
        print(f"Arkusz 2 wewnƒôtrzne duplikaty: {len(df2_internal_dupes):,}")

        # Sprawd≈∫ nak≈Çadanie MIƒòDZY arkuszami
        df1_unique_keys = set(df1['full_transaction_key'].unique())
        df2_unique_keys = set(df2['full_transaction_key'].unique())
        common_keys = df1_unique_keys.intersection(df2_unique_keys)

        print(f"\nDUPLIKATY POMIƒòDZY ARKUSZAMI:")
        print(f"Arkusz 1 unikalne klucze: {len(df1_unique_keys):,}")
        print(f"Arkusz 2 unikalne klucze: {len(df2_unique_keys):,}")
        print(f"Wsp√≥lne klucze miƒôdzy arkuszami: {len(common_keys):,}")

        # Zlicz ile rekord√≥w odpowiada wsp√≥lnym kluczom
        df1_common_records = df1[df1['full_transaction_key'].isin(common_keys)]
        df2_common_records = df2[df2['full_transaction_key'].isin(common_keys)]

        print(f"Liczba rekord√≥w z wsp√≥lnymi kluczami w arkuszu 1: {len(df1_common_records):,}")
        print(f"Liczba rekord√≥w z wsp√≥lnymi kluczami w arkuszu 2: {len(df2_common_records):,}")

        # POPRAWNA KONSOLIDACJA
        print(f"\nPOPRAWNA STRATEGIA KONSOLIDACJI:")

        # Krok 1: Usu≈Ñ duplikaty wewnƒôtrzne w ka≈ºdym arkuszu
        df1_clean = df1.drop_duplicates(subset=['full_transaction_key'], keep='first')
        df2_clean = df2.drop_duplicates(subset=['full_transaction_key'], keep='first')

        print(f"Po wewnƒôtrznej deduplikacji:")
        print(f"Arkusz 1: {len(df1):,} ‚Üí {len(df1_clean):,} (usuniƒôto {len(df1) - len(df1_clean):,})")
        print(f"Arkusz 2: {len(df2):,} ‚Üí {len(df2_clean):,} (usuniƒôto {len(df2) - len(df2_clean):,})")
                                        
        # Krok 2: Po≈ÇƒÖcz i usu≈Ñ duplikaty miƒôdzy arkuszami
        df_combined = pd.concat([df1_clean, df2_clean], ignore_index=True)
        df_final = df_combined.drop_duplicates(subset=['full_transaction_key'], keep='first')

        print(f"\nPo konsolidacji miƒôdzy arkuszami:")
        print(f"Przed merge: {len(df_combined):,}")
        print(f"Po merge: {len(df_final):,}")
        print(f"Liczba usuniƒôtych duplikat√≥w miƒôdzy arkuszami: {len(df_combined) - len(df_final):,}")

        # PODSUMOWANIE LOGICZNE 
        total_internal_removed = (len(df1) - len(df1_clean)) + (len(df2) - len(df2_clean))
        between_sheets_removed = len(df_combined) - len(df_final)
        total_removed = total_internal_removed + between_sheets_removed

        print(f"\nPODSUMOWANIE KO≈ÉCOWE:")
        print(f"Usuniƒôto duplikat√≥w wewnƒôtrznych: {total_internal_removed:,}")
        print(f"Usuniƒôto duplikat√≥w miƒôdzy arkuszami: {between_sheets_removed:,}")
        print(f"≈ÅƒÑCZNIE usuniƒôto duplikat√≥w: {total_removed:,}")
        print(f"Wsp√≥lne klucze miƒôdzy arkuszami: {len(common_keys):,}")

        # Sprawd≈∫ czy logika jest sp√≥jna
        if between_sheets_removed == len(common_keys):
            print("LOGIKA POPRAWNA: Usuniƒôte duplikaty = wsp√≥lne klucze")
        else:
            print("SPRAWD≈π: Logika wymaga dalszej analizy")

        # Finalny dataset
        df = df_final.drop('full_transaction_key', axis=1)
        
   
    # Konwersja datetime
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

    print(f"\nFINALNY DATASET:")
    print(f"Rekord√≥w: {len(df):,}")
    print(f"Kolumny: {list(df.columns)}")
    print(f"Rozmiar: {df.memory_usage(deep=True).sum() / 1014**2:.1f} MB")
    print(f"Okres: {df['InvoiceDate'].min()} ‚Üí {df['InvoiceDate'].max()}")
    print("GOTOWE DO ANALIZY!")
    
except FileNotFoundError:
    print("B≈ÅƒÑD: Brak pliku online_retail_II.xlsx")
    print("Pobierz z: https://archive.ics.uci.edu/ml/datasets/Online+Retail+II")
except Exception as e:
    print(f"B≈ÅƒÑD: {str(e)}")



# =================================
# ANALIZA JAKO≈öCI PO KONSOLIDACJI
# ==================================

print("KONTROLA JAKO≈öCI PO KONSOLIDACJI")
print("=" * 35)

# Analiza rozk≈Çadu dat miƒôdzy arkuszami (je≈õli to by≈Ç podzia≈Ç czasowy)
print("ROZK≈ÅAD CZASOWY:")
df['Year'] = df['InvoiceDate'].dt.year
df['Month'] = df['InvoiceDate'].dt.month
yearly_dist = df.groupby('Year').size()
print("Rozk≈Çad roczny:")
for year, count in yearly_dist.items():
    print(f"  {year}: {count:,} rekord√≥w")

# Sprawd≈∫, czy by≈Ça logika podzia≈Çu arkuszy
if len(sheet_names) == 2:
    # Przyk≈Çady mo≈ºliwych krtyteri√≥w podzia≈Çu:
    print(f"\nMO≈ªLIWE KRYTERIA PODZIA≈ÅU ARKUSZY:")

    # Podzia≈Ç czasowy?
    df1_years = set(sheets_data[sheet1_name]['InvoiceDate'].dt.year)
    df2_years = set(sheets_data[sheet2_name]['InvoiceDate'].dt.year)
    if df1_years != df2_years:
        print(f"* Podzia≈Ç czasowy: Arkusz 1 ({list(df1_years)}) vs Arkusz 2 ({list(df2_years)}")

    # Podzia≈Ç geograficzny?
    df1_countries = set(sheets_data[sheet1_name]['Country'].unique())
    df2_countries = set(sheets_data[sheet2_name]['Country'].unique())
    common_countries = df1_countries.intersection(df2_countries)
    if len(common_countries) < min(len(df1_countries), len(df2_countries)):
        print(f"* Czƒô≈õciowy podzia≈Ç geograficzny: {len(common_countries)} wsp√≥lnych kraj√≥w")

    print(f"\nPO KONSOLIDACJI - METRYKI DATASET:")
    print(f"* ≈ÅƒÖczne unikalne faktury: {df['Invoice'].nunique():,}")
    print(f"* ≈ÅƒÖczni unikalni Klienci: {df['Customer ID'].nunique():,}")
    print(f"* ≈ÅƒÖczne unikalne produkty: {df['StockCode'].nunique():,}")
    print(f"* Jako≈õƒá danych po konsolidacji - weryfikacja w kolejnych kom√≥rkach/krokach")



# ==========================================
# ANALIZA STRUKTURY I BASIC STATS
# ==========================================

print("PODSTAWOWA ANALIZA STRUKTURY")
print("=" * 40)

# Info o typach danych
print("TYPY DANYCH I PAMIƒòƒÜ:")
df.info()

print("\n" + "="*40)
print("STATYSTYKI NUMERYCZNE:")
display(df.describe())

print("\n" + "="*40) 
print("UNIKALNE WARTO≈öCI PER KOLUMNA:")
for col in df.columns:
    unique_count = df[col].nunique()
    null_count = df[col].isnull().sum()
    null_pct = (null_count / len(df)) * 100
    print(f"‚Ä¢ {col:<15}: {unique_count:>8,} unique | {null_count:>8,} nulls ({null_pct:>5.1f}%)")

# Sample data preview
print("\n" + "="*40)
print("PIERWSZE 5 REKORD√ìW:")
display(df.head())



# ============================================
# ANALIZA BRAKUJƒÑCYCH DANYCH I WIZUALIZACJA
# ============================================

print("ANALIZA BRAKUJƒÑCYCH DANYCH")
print("=" * 35)

# Missing data summary
missing_data = df.isnull().sum()
missing_percent = (missing_data / len(df)) * 100
missing_summary = pd.DataFrame({
    'BrakujƒÖce': missing_data,
    'Procent': missing_percent
}).sort_values('Procent', ascending=False)

print("PODSUMOWANIE MISSING DATA:")
display(missing_summary[missing_summary['BrakujƒÖce'] > 0])

# VIsualtization
plt.figure(figsize=(12, 6))
missing_summary[missing_summary['Procent'] > 0]['Procent'].plot(kind='bar', color='red', alpha=0.7)
plt.title('Procent brakujƒÖcych danych w kolumnach', fontsize=14, pad=20)
plt.ylabel('Procent (%)')
plt.xlabel('Kolumny')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Business impact analysis
print("\n WP≈ÅYW BIZNESOWY:")
total_records = len(df)
customer_missing = missing_data['Customer ID']
print(f"* Guest purchases (brak Customer ID): {customer_missing:,} ({customer_missing / total_records * 100:.1f}%)")
print(f"* Impact: Ograniczona analiza customer lifetime value")
print(f"* Rekomendacja: Osobna analiza guest vs registered customers")


# ======================================
# PROBLEMY JAKO≈öCIOWE I REGU≈ÅY BIZNESOWE
# ======================================

print("IDENTYFIKACJA PROBLEM√ìW JAKO≈öCIOWYCH")
print("=" * 35)

# Returns/Cancellations (negative quantities)
negative_qty = df[df['Quantity'] < 0]
print(f"Zwroty/Anulowania: {len(negative_qty):,} ({len(negative_qty)/len(df)*100:.1f}%)")

# Zero prices
zero_price = df[df['Price'] == 0]
print(f"Ceny zerowe: {len(zero_price):,} ({len(zero_price)/len(df)*100:.1f}%)")

# Extreme values
extreme_qty = df[df['Quantity'] > 1000]
extreme_price = df[df['Price'] > 1000]
print(f"Ekstremalne ilo≈õci (>1000): {len(extreme_qty):,}")
print(f"Produkty wysokiej warto≈õci (>¬£1000): {len(extreme_price):,}")

# Calculate total transaction value
df['Total_Value'] = df['Quantity'] * df['Price']

# Top problematic transactions
print("\nNAJWA≈ªNIEJSZE PRZYPADKI PROBLEMOWE:")
print("Pr√≥bki cen zerowych:")
display(zero_price[['Invoice', 'StockCode', 'Description', 'Quantity', 'Price']].head(3))

print("\nPr√≥bki ekstremalnych ilo≈õci:")
display(extreme_qty[['Invoice', 'StockCode', 'Description', 'Quantity', 'Price']].head(3))

# Business rules recommendations
print("\n\n REKOMENDACJE:")
print("* Zdefiniuj obs≈Çugƒô guest purchases (Customer ID = null)")
print("* Oddziel zwroty (negative Quantity) od g≈Ç√≥wnej analizy sprzeda≈ºy")
print("* Zbadaj ceny zerowe - produkty promocyjne vs b≈Çƒôdy danych")
print("* Ustaw progi dla walidacji warto≈õci ekstremalnych")
print("* Wyczy≈õƒá opisy produkt√≥w dla sp√≥jnej kategoryzacji")
      


# ===========================================
# TRENDY W CZASIE I KLUCZOWE SPOSTRZE≈ªENIA
# ===========================================

print("TRENDY CZASOWE I KLUCZOWE SPOSTRZE≈ªENIA")
print("=" * 40)

# Miesiƒôczny trend sprzeda≈ºy
monthly_sales = df[df['Quantity'] > 0].groupby(df['InvoiceDate'].dt.to_period('M')).agg({
    'Total_Value': 'sum',
    'Invoice': 'nunique',
    'Customer ID': 'nunique',
    'Quantity': 'sum'
}).round(2)

print("TRENDY MIESIƒòCZNE (tylko pozytywne ilo≈õci):")
display(monthly_sales.head(8))

# G≈Ç√≥wne miary
positive_df = df[df['Quantity'] > 0]
print(f"\nKPI:")
print(f"* ≈ÅƒÖczne przychody ze sprzeda≈ºy: ¬£{positive_df['Total_Value'].sum():,.2f}")
print(f"* ≈örednia warto≈õƒá zam√≥wienia: ¬£{positive_df.groupby('Invoice') ['Total_Value'].sum().mean():.2f}")
print(f"* Unikalni Klienci: {positive_df['Customer ID'].nunique():,}")
print(f"* Unikalne produkty: {positive_df['StockCode'].nunique():,}")
print(f"* Obs≈Çugiwane kraje: {positive_df['Country'].nunique()}")

# Ocena jako≈õci danych
total_checks = 5    # brakujƒÖce dane, zera, warto≈õci ujemne lub ekstremalne
passed_checks = 0
if missing_data['Customer ID'] / len(df) < 0.3: passed_checks += 1  # <30% brak√≥w - OK
if len(negative_qty) / len(df) < 0.1: passed_checks += 1  # <10% zwrot√≥w - OK
if len(zero_price) / len(df) < 0.05: passed_checks += 1  # <5% cen zerowych - OK
if len(extreme_qty) / len(df) < 0.01: passed_checks += 1  # <1% warto≈õci ekstremalnych - OK
if df['InvoiceDate'].isnull().sum() == 0: passed_checks +=1  # brak brakujƒÖcych dat - OK

quality_score = (passed_checks / total_checks) * 100
print(f"\nOCENA JAKO≈öCI DANYCH: {quality_score:.0f}% ({passed_checks}/{total_checks} test√≥w zaliczonych)") 

print(f"\nDATASET GOTOWY DO MODELU GWIAZDY:")
print(f"* Fact_Sales: {len(positive_df):,} prawid≈Çowych transakcji")
print(f"* Dim_Customer: ~{positive_df['Customer ID'].nunique():,} + segment Go≈õci")
print(f"* Dim_Product: {positive_df['StockCode'].nunique():,} unikalnych produkt√≥w")
print(f"* Dim_Date: {(positive_df['InvoiceDate'].max() - positive_df['InvoiceDate'].min()).days} dni zakresu danych")



